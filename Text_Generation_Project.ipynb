{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Generation_Project.ipynb","provenance":[{"file_id":"1vZRcexgZD-QcBUrpt4CJqAJNPsHwBo2H","timestamp":1589113476538}],"collapsed_sections":[],"authorship_tag":"ABX9TyOiBrFtBWGF7R7EdmoB6yxP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"rpKCJ8preB9f","colab_type":"code","outputId":"3f5aff13-143f-4473-be5a-f36287bc91b8","executionInfo":{"status":"ok","timestamp":1589208044509,"user_tz":-330,"elapsed":4655,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# importing the dependencies\n","import numpy \n","import sys\n","import keras\n","import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","from keras.utils import np_utils\n","from keras.callbacks import ModelCheckpoint"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OvfD4GOqjpU3","colab_type":"code","outputId":"bd93cc5a-b7fa-4736-8858-c78fe4e995bb","executionInfo":{"status":"ok","timestamp":1589208044509,"user_tz":-330,"elapsed":4649,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jlITDAfqeeDr","colab_type":"code","colab":{}},"source":["# load data\n","# loading the data and opening our input data in the form of a txt file\n","# Project Gutenburg/berg is where the data can be found (google it)\n","with open('/content/drive/My Drive/Colab Notebooks/Frankenstein.txt', 'r') as docs: \n","     file1 = docs.read() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1aA53R6jlUsW","colab_type":"code","colab":{}},"source":["# tokenization \n","# standardization\n","# what is Tokenization? Tokenization is the process of breaking a stream of text up into words phases symbols or a meaningful elements\n","def tokenize_words(input):\n","    # lowercase everything to standardize it\n","    input = input.lower()\n","    # instantiating the tonkenizer\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    # tokenizing the text into tokens\n","    tokens = tokenizer.tokenize(input)\n","    # filtering the stopwords using the lambda\n","    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n","    return \"\".join(filtered)  \n","# preprocess the input data, make tokens\n","processed_inputs = tokenize_words(file1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jBZaGzJmTiJ","colab_type":"code","colab":{}},"source":["# chars to numbers\n","# convert characters in our input to numbers \n","# we'll sort the list of the set all characters that appears in out i/p text and then use the enumerate fn to get numbers that represents the characters\n","# we'll thencreat a dictionary that stores the keys and values, or the characters and the number the repesent them\n","chars = sorted (list(set(processed_inputs)))\n","chars_to_num = dict((c,i) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHCdi7r2ujFg","colab_type":"code","outputId":"9025e443-0fa0-4e61-a652-b710a87cfb55","executionInfo":{"status":"ok","timestamp":1589208050883,"user_tz":-330,"elapsed":11000,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# check if words to chars or chars to num (?!) has worked?\n","# just so we get an idea of whether our process of converting words to characters has worked\n","# we jsut print the length of our variables\n","input_len = len(processed_inputs)\n","vocab_len = len(chars)\n","print(\"Total no. of characters: \",input_len)\n","print(\"Total vocab: \", vocab_len)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Total no. of characters:  241873\n","Total vocab:  42\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"llg8AvqovUzU","colab_type":"code","colab":{}},"source":["# seq_length\n","# we're defining how long we want an individual sequence here\n","# an individual sequence is a complete mapping of input characeters as integers\n","seq_length = 100\n","x_data = []\n","y_data = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oh5l8gFBvj5s","colab_type":"code","outputId":"ea20235b-10f3-48c3-c79c-d9b212d912a1","executionInfo":{"status":"ok","timestamp":1589208053396,"user_tz":-330,"elapsed":13500,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# loop through the sequence\n","# here we' re goining through the entire list pf i/p and converting the chars to numbers with a for loop\n","# this will create a bunch of sequence where each sequence starts with the next character in the i/p data\n","# begining with the first character \n","for i in range(0, input_len - seq_length,1):\n","    #define the i/p and o/p of sqeuences\n","    # i/p is the current character plus total sequence length \n","    in_seq = processed_inputs[i:i + seq_length]\n","    #out sequence is the initial character plus total sequence length\n","    out_seq = processed_inputs[i + seq_length]\n","    #converting the list of characters to integers based on previous values and appending the values to our lists\n","    x_data.append([chars_to_num[char] for char in in_seq])\n","    y_data.append(chars_to_num[out_seq])\n","\n","#check to see how many total input sequences we have\n","n_patterns = len(x_data)\n","print(\"TOtal Patterns\", n_patterns)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["TOtal Patterns 241773\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HfFU9W3swpV9","colab_type":"code","colab":{}},"source":["#convert input sequence to np array that our network can use\n","X = numpy.reshape(x_data, (n_patterns, seq_length,1))\n","X = X/float(vocab_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxiD4sFlxWj-","colab_type":"code","colab":{}},"source":["# one-hot encoding our label\n","y = np_utils.to_categorical(y_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-ZAXm6px6JH","colab_type":"code","colab":{}},"source":["# creating the sequencial model\n","# creating a sequencial model\n","# dropout is used to prevent overfitting\n","model = Sequential()\n","model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TM_owQ4Cy9Pa","colab_type":"code","colab":{}},"source":["# comppile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"folnzMThzM_h","colab_type":"code","colab":{}},"source":["# saving weights\n","filepath ='model_weights_saved.hdf5'\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","desired_callbacks = [checkpoint]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bu-E9H5nz4eO","colab_type":"code","outputId":"042a084f-a839-4d64-9939-e5932969dae9","colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"status":"ok","timestamp":1589207967061,"user_tz":-330,"elapsed":127767,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}}},"source":["# fit model and let it train\n","model.fit(X,y, epochs=4,  batch_size=256, callbacks=desired_callbacks)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/4\n","241773/241773 [==============================] - 4075s 17ms/step - loss: 2.9366\n","\n","Epoch 00001: loss improved from inf to 2.93664, saving model to model_weights_saved.hdf5\n","Epoch 2/4\n","241773/241773 [==============================] - 4120s 17ms/step - loss: 2.9154\n","\n","Epoch 00002: loss improved from 2.93664 to 2.91542, saving model to model_weights_saved.hdf5\n","Epoch 3/4\n","241773/241773 [==============================] - 4125s 17ms/step - loss: 2.9061\n","\n","Epoch 00003: loss improved from 2.91542 to 2.90606, saving model to model_weights_saved.hdf5\n","Epoch 4/4\n","241773/241773 [==============================] - 4143s 17ms/step - loss: 2.8768\n","\n","Epoch 00004: loss improved from 2.90606 to 2.87682, saving model to model_weights_saved.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fe34c8f4f28>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"aDm1TzBP5i8D","colab_type":"code","colab":{}},"source":["# recompile model with the saved weigths\n","filename = \"model_weights_saved.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZrqS-1n7bk4","colab_type":"code","colab":{}},"source":["# output of the model back into characters\n","num_to_char = dict((i,c) for i,c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NP9vJknh7s8C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"164ff921-2d4e-4b8e-c080-417099fed062","executionInfo":{"status":"ok","timestamp":1589208562154,"user_tz":-330,"elapsed":7420,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}}},"source":["#random seed to help generate\n","start = numpy.random.randint(0, len(x_data) - 1)\n","pattern = x_data[start]\n","print(\"Random seed: \")\n","print(\"\\\"\",''.join([num_to_char[value] for value in pattern]), \"\\\"\")"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Random seed: \n","\" ughtinformationkrempegivenconcerninglecturesalthoughcouldconsentgohearlittleconceitedfellowdeliverse \"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p3KnBCcZ9Bqq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"ee0dd654-2270-4852-88d8-20371391615d","executionInfo":{"status":"ok","timestamp":1589208662590,"user_tz":-330,"elapsed":33386,"user":{"displayName":"Ritwik Shirbhate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYE6nGcXUDVFYbcuH2EPnVRtvr86aawuEkYiOd=s64","userId":"05781157606724727180"}}},"source":["# generate the text \n","for i in range (1000):\n","  x = numpy.reshape(pattern,(1,len(pattern),1))\n","  x = x/float(vocab_len)\n","  prediction = model.predict(x, verbose = 0)\n","  index = numpy.argmax(prediction)\n","  result = num_to_char[index]\n","  seq_in = [num_to_char[value] for value in pattern]\n","  sys.stdout.write(result)\n","  pattern.append(index)\n","  pattern = pattern [1:len(pattern)]"],"execution_count":38,"outputs":[{"output_type":"stream","text":["reerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereere"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z8fs0TET-kgC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}